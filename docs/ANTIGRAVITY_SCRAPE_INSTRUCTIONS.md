# üöÄ INSTRUCCIONES COMPLETAS - ANTIGRAVITY
## PROYECTO: SCRAPE MASIVO VENUZ - PORNDUDE + TOP 10 SITIOS

**Fecha:** 2026-01-28  
**Objetivo:** Poblar BD VENUZ con 5000+ registros de sitios adultos  
**Tiempo Estimado:** 6-8 horas  
**Responsable:** Antigravity (Remote Execution)

---

## üìã CONTEXTO DEL PROYECTO

### **Visi√≥n VENUZ**
VENUZ es una plataforma de descubrimiento adulto tipo TikTok con:
- Feed infinito inteligente
- M√∫ltiples categor√≠as organizadas
- Links de afiliados
- Tono "gu√≠a tur√≠stica" (NO hard-sell)
- Escalable: nuevos sitios continuamente

### **Tu Tarea**
Scrape + Estructura + Insertar datos de PornDude + Top 10 sitios web en Supabase.

---

## üéØ ESTRUCTURA PORNDUDE vs VENUZ

### **PORNDUDE (REFERENCIA)**
- Organizacion por categor√≠as (webcams, escorts, clubs, etc)
- Descripciones detalladas
- Links directos
- Fotos/thumbnails
- Ratings y reviews

### **VENUZ (OBJETIVO)**
```json
{
  "id": "unique-id",
  "title": "Nombre del lugar/modelo",
  "description": "Descripci√≥n en tono gu√≠a tur√≠stica",
  "image_url": "url_imagen_atractiva",
  "video_url": "url_video_preview (opcional)",
  "category": "webcam|escort|club|bar|servicio|evento|concierto|citas|otro",
  "subcategory": "subtipo dentro categor√≠a",
  "location": "Ciudad, Pa√≠s",
  "latitude": 20.6534,
  "longitude": -105.2253,
  "affiliate_url": "link_actual_temporal",
  "affiliate_source": "porndude|camsoda|stripchat|chaturbate|otro",
  "is_verified": true|false,
  "is_premium": true|false,
  "is_open_now": true|false,
  "open_until": "4:00 AM",
  "rating": 4.5,
  "likes": 234,
  "views": 1523,
  "viewers_now": 847,
  "address": "Direcci√≥n f√≠sica si aplica",
  "phone": "Tel√©fono si aplica",
  "active": true,
  "created_at": "2026-01-28T15:30:00Z",
  "updated_at": "2026-01-28T15:30:00Z"
}
```

---

## üìÇ ESTRUCTURA DE CARPETAS

Crea esto en tu sistema:

```
C:\Users\pablo\Downloads\VENUZ-Complete-App\venuz-app\scrape-data\
‚îú‚îÄ checkpoint.json                    ‚Üê ESTADO ACTUAL (lee primero!)
‚îú‚îÄ CHECKPOINT_HISTORY.json            ‚Üê Hist√≥rico
‚îú‚îÄ 001_webcams.json                   ‚Üê Datos por categor√≠a
‚îú‚îÄ 002_escorts.json
‚îú‚îÄ 003_clubs.json
‚îú‚îÄ 004_bares.json
‚îú‚îÄ 005_servicios.json
‚îú‚îÄ 006_eventos.json
‚îú‚îÄ 007_conciertos.json
‚îú‚îÄ 008_citas.json
‚îú‚îÄ 009_apps_citas.json
‚îú‚îÄ 010_otros.json
‚îú‚îÄ SCRAPE_LOG.txt                     ‚Üê Log detallado
‚îú‚îÄ FINAL_DATA.json                    ‚Üê Consolidado listo para insertar
‚îî‚îÄ README_SCRAPE.md                   ‚Üê Este archivo
```

---

## üîç TOP 10 SITIOS A SCRAPEAR

**Prioridad 1 (CR√çTICO):**
1. **PornDude.com** - Directorio maestro
2. **CamSoda.com** - Webcams (API available)
3. **Stripchat.com** - Webcams
4. **Chaturbate.com** - Webcams

**Prioridad 2 (IMPORTANTE):**
5. **Encontrales.net** - Escorts Mexico
6. **Sexomercado.com** - Escorts Mexico
7. **BeautifulAgency.com** - Escorts premium

**Prioridad 3 (NICE TO HAVE):**
8. **Google Places API** - Clubs, bares
9. **TikTok/Instagram** - Trending content
10. **Sitios locales mexicanos** - Eventos, conciertos

---

## üíª SCRIPT PYTHON - SCRAPER PRINCIPAL

```python
#!/usr/bin/env python3
# scraper.py - ANTIGRAVITY SCRAPER

import json
import os
import time
from datetime import datetime
from typing import List, Dict, Any
import requests
from bs4 import BeautifulSoup
import logging

# ============================================
# CONFIGURACION
# ============================================

SCRAPE_DATA_DIR = r"C:\Users\pablo\Downloads\VENUZ-Complete-App\venuz-app\scrape-data"
CHECKPOINT_FILE = os.path.join(SCRAPE_DATA_DIR, "checkpoint.json")
LOG_FILE = os.path.join(SCRAPE_DATA_DIR, "SCRAPE_LOG.txt")

# Crear directorio si no existe
os.makedirs(SCRAPE_DATA_DIR, exist_ok=True)

# Logging
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# ============================================
# ESTRUCTURA DE CHECKPOINT
# ============================================

DEFAULT_CHECKPOINT = {
    "timestamp": datetime.now().isoformat(),
    "estado": "INICIADO",
    "categorias_completadas": {},
    "total_registros_scrapeados": 0,
    "proxima_categoria": "webcams",
    "proxima_url": None,
    "errores": []
}

# ============================================
# FUNCIONES HELPER
# ============================================

def crear_carpeta_scrape():
    """Crea carpeta de scrape si no existe"""
    os.makedirs(SCRAPE_DATA_DIR, exist_ok=True)
    print(f"‚úÖ Carpeta creada: {SCRAPE_DATA_DIR}")

def leer_checkpoint():
    """Lee checkpoint anterior (para continuar si se corta)"""
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
            checkpoint = json.load(f)
        print(f"‚úÖ Checkpoint cargado: {checkpoint['proxima_categoria']}")
        return checkpoint
    else:
        print("‚ÑπÔ∏è Primer run - Iniciando desde cero")
        return DEFAULT_CHECKPOINT.copy()

def guardar_checkpoint(checkpoint: Dict[str, Any]):
    """Guarda checkpoint cada 100 registros"""
    checkpoint["timestamp"] = datetime.now().isoformat()
    with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
        json.dump(checkpoint, f, indent=2, ensure_ascii=False)
    logging.info(f"üíæ Checkpoint guardado - Total: {checkpoint['total_registros_scrapeados']}")

def guardar_datos_categoria(categoria: str, datos: List[Dict]):
    """Guarda datos de cada categor√≠a en archivo separado"""
    archivo = os.path.join(SCRAPE_DATA_DIR, f"{categoria}.json")
    with open(archivo, 'w', encoding='utf-8') as f:
        json.dump(datos, f, indent=2, ensure_ascii=False)
    logging.info(f"üíæ {len(datos)} registros guardados en {categoria}.json")

def reporte_progreso(checkpoint: Dict[str, Any]):
    """Imprime reporte de progreso"""
    print("\n" + "="*60)
    print("üìä REPORTE DE PROGRESO")
    print("="*60)
    print(f"Total registros scrapeados: {checkpoint['total_registros_scrapeados']}")
    print(f"Pr√≥xima categor√≠a: {checkpoint['proxima_categoria']}")
    print(f"Categor√≠as completadas: {list(checkpoint['categorias_completadas'].keys())}")
    if checkpoint['errores']:
        print(f"‚ö†Ô∏è Errores encontrados: {len(checkpoint['errores'])}")
    print("="*60 + "\n")

# ============================================
# SCRAPERS POR SITIO
# ============================================

class PornDudeScraper:
    """Scraper para PornDude.com"""
    
    def __init__(self):
        self.base_url = "https://www.porndude.com"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def scrape_webcams(self) -> List[Dict]:
        """Scrape categor√≠a webcams"""
        datos = []
        try:
            # Estructura esperada de PornDude
            url = f"{self.base_url}/en/porn-sites/live-sex-cams"
            response = requests.get(url, headers=self.headers, timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Buscar items (estructura puede variar)
            items = soup.find_all('div', class_='item')  # Ajustar selector
            
            for item in items[:50]:  # Primeros 50 de prueba
                try:
                    title = item.find('h3')
                    img = item.find('img')
                    desc = item.find('p')
                    link = item.find('a')
                    
                    if title and link:
                        datos.append({
                            "title": title.text.strip(),
                            "description": desc.text.strip() if desc else "Sitio de webcams en vivo",
                            "image_url": img.get('src') if img else "",
                            "affiliate_url": link.get('href'),
                            "affiliate_source": "porndude",
                            "category": "webcam",
                            "is_verified": True,
                            "is_premium": False,
                            "rating": 4.5,
                            "active": True,
                            "created_at": datetime.now().isoformat()
                        })
                except Exception as e:
                    logging.error(f"Error parseando item: {e}")
                    continue
            
            logging.info(f"‚úÖ PornDude webcams: {len(datos)} registros")
            return datos
            
        except Exception as e:
            logging.error(f"‚ùå Error en PornDude webcams: {e}")
            return []
    
    def scrape_escorts(self) -> List[Dict]:
        """Scrape categor√≠a escorts"""
        datos = []
        try:
            url = f"{self.base_url}/en/porn-sites/escort-sites"
            response = requests.get(url, headers=self.headers, timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.content, 'html.parser')
            
            items = soup.find_all('div', class_='item')
            
            for item in items[:50]:
                try:
                    title = item.find('h3')
                    img = item.find('img')
                    desc = item.find('p')
                    link = item.find('a')
                    
                    if title and link:
                        datos.append({
                            "title": title.text.strip(),
                            "description": desc.text.strip() if desc else "Plataforma de escorts verificadas",
                            "image_url": img.get('src') if img else "",
                            "affiliate_url": link.get('href'),
                            "affiliate_source": "porndude",
                            "category": "escort",
                            "is_verified": True,
                            "is_premium": True,
                            "rating": 4.7,
                            "active": True,
                            "created_at": datetime.now().isoformat()
                        })
                except Exception as e:
                    logging.error(f"Error parseando escort: {e}")
                    continue
            
            logging.info(f"‚úÖ PornDude escorts: {len(datos)} registros")
            return datos
            
        except Exception as e:
            logging.error(f"‚ùå Error en PornDude escorts: {e}")
            return []
    
    def scrape_clubs(self) -> List[Dict]:
        """Scrape categor√≠a clubs"""
        # Similar a webcams/escorts
        return []
    
    def scrape_bares(self) -> List[Dict]:
        """Scrape categor√≠a bares"""
        return []

class CamSodaScraper:
    """Scraper para CamSoda.com (con API si est√° disponible)"""
    
    def scrape_live_models(self) -> List[Dict]:
        """Scrape modelos en vivo de CamSoda"""
        datos = []
        try:
            # CamSoda podr√≠a tener API p√∫blica
            url = "https://www.camsoda.com/api/v1/browse/recommendations"
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                json_data = response.json()
                # Procesar respuesta seg√∫n estructura API
                logging.info(f"‚úÖ CamSoda: {len(datos)} modelos en vivo")
            
        except Exception as e:
            logging.error(f"‚ùå Error CamSoda: {e}")
        
        return datos

# ============================================
# TRANSFORMACION DE DATOS - TONO GU√çA TUR√çSTICA
# ============================================

TRANSFORMACIONES = {
    "webcam": {
        "gen√©rico": "Visita este sitio de entretenimiento en vivo",
        "premium": "Descubre contenido premium de modelaje en vivo",
        "trending": "Sitio trending con shows en vivo ahora"
    },
    "escort": {
        "gen√©rico": "Conecta con profesionales independientes verificadas",
        "premium": "Acceso a servicios VIP con modelos certificadas",
        "trending": "Compa√±√≠a profesional - Disponible 24/7"
    },
    "club": {
        "gen√©rico": "Disfruta de la mejor vida nocturna",
        "premium": "Club VIP exclusivo con ambiente premium",
        "trending": "Club trending - Vive la experiencia nocturna"
    },
    "bar": {
        "gen√©rico": "Buen lugar para disfrutar y socializar",
        "premium": "Bar premium con ambiente sofisticado",
        "trending": "Bar trending - Ambiente vibrante"
    }
}

def transformar_descripcion(categoria: str, es_premium: bool, es_trending: bool) -> str:
    """Transforma descripci√≥n a tono 'gu√≠a tur√≠stica'"""
    if categoria not in TRANSFORMACIONES:
        return "Descubre este lugar interesante"
    
    cat_transforms = TRANSFORMACIONES[categoria]
    
    if es_trending:
        return cat_transforms.get("trending", cat_transforms["gen√©rico"])
    elif es_premium:
        return cat_transforms.get("premium", cat_transforms["gen√©rico"])
    else:
        return cat_transforms.get("gen√©rico")

# ============================================
# INSERCI√ìN EN SUPABASE
# ============================================

def insertar_en_supabase(datos: List[Dict]):
    """Inserta datos en Supabase"""
    try:
        from supabase import create_client, Client
        
        # Variables de entorno
        supabase_url = "https://jbrmziwosyeructvlvrq.supabase.co"
        supabase_key = os.getenv("SUPABASE_KEY")  # Configurar en .env
        
        if not supabase_key:
            logging.error("‚ùå SUPABASE_KEY no configurada")
            return False
        
        supabase: Client = create_client(supabase_url, supabase_key)
        
        # Insertar en batch de 100
        for i in range(0, len(datos), 100):
            batch = datos[i:i+100]
            response = supabase.table('content').insert(batch).execute()
            logging.info(f"‚úÖ Insertados {len(batch)} registros en Supabase")
        
        return True
        
    except Exception as e:
        logging.error(f"‚ùå Error insertando en Supabase: {e}")
        return False

# ============================================
# MAIN EXECUTION
# ============================================

def main():
    print("üöÄ INICIANDO SCRAPE MASIVO VENUZ")
    print("="*60)
    
    # 1. Crear carpeta
    crear_carpeta_scrape()
    
    # 2. Leer checkpoint (para continuar si se corta)
    checkpoint = leer_checkpoint()
    
    # 3. Scraping
    print("\nüìç FASE 1: SCRAPEANDO PORNDUDE")
    scraper_pd = PornDudeScraper()
    
    datos_webcams = scraper_pd.scrape_webcams()
    guardar_datos_categoria("001_webcams", datos_webcams)
    checkpoint['categorias_completadas']['webcams'] = len(datos_webcams)
    checkpoint['total_registros_scrapeados'] += len(datos_webcams)
    guardar_checkpoint(checkpoint)
    
    datos_escorts = scraper_pd.scrape_escorts()
    guardar_datos_categoria("002_escorts", datos_escorts)
    checkpoint['categorias_completadas']['escorts'] = len(datos_escorts)
    checkpoint['total_registros_scrapeados'] += len(datos_escorts)
    guardar_checkpoint(checkpoint)
    
    # 4. CamSoda
    print("\nüìç FASE 2: SCRAPEANDO CAMSODA")
    scraper_cs = CamSodaScraper()
    datos_camsoda = scraper_cs.scrape_live_models()
    guardar_datos_categoria("001_webcams", datos_camsoda)
    
    # 5. Consolidar
    print("\nüìç FASE 3: CONSOLIDANDO DATOS")
    todos_datos = datos_webcams + datos_escorts + datos_camsoda
    
    # Guardar consolidado
    final_file = os.path.join(SCRAPE_DATA_DIR, "FINAL_DATA.json")
    with open(final_file, 'w', encoding='utf-8') as f:
        json.dump(todos_datos, f, indent=2, ensure_ascii=False)
    
    # 6. Reporte final
    reporte_progreso(checkpoint)
    
    print(f"\n‚úÖ SCRAPE COMPLETO")
    print(f"üìä Total registros: {len(todos_datos)}")
    print(f"üìÅ Datos guardados en: {SCRAPE_DATA_DIR}")
    print(f"üìÑ Archivo final: {final_file}")
    
    # 7. Insertar en Supabase (opcional)
    print("\nüîÑ ¬øInsertar en Supabase? (manual o autom√°tico)")

if __name__ == "__main__":
    main()
```

---

## üîß INSTALACI√ìN & EJECUCI√ìN

### **1. Requisitos**
```bash
pip install requests beautifulsoup4 supabase-py python-dotenv
```

### **2. Configurar variables de entorno**
```
# .env
SUPABASE_KEY=tu_supabase_key_aqui
SUPABASE_URL=https://jbrmziwosyeructvlvrq.supabase.co
```

### **3. Ejecutar scraper**
```powershell
cd C:\Users\pablo\Downloads\VENUZ-Complete-App\venuz-app
python scraper.py
```

### **4. Monitorear progreso**
```powershell
# Ver log en tiempo real
Get-Content scrape-data\SCRAPE_LOG.txt -Tail 20 -Wait

# Ver checkpoint
Get-Content scrape-data\checkpoint.json
```

---

## üìä INSERCI√ìN EN SUPABASE

Una vez scraped, insertar con este script SQL:

```sql
-- INSERT_SCRAPED_DATA.sql
BEGIN;

-- Insertar datos scrapeados
INSERT INTO content (
  title, description, image_url, video_url,
  category, subcategory, location,
  latitude, longitude,
  affiliate_url, affiliate_source,
  is_verified, is_premium,
  rating, likes, views,
  active, created_at, updated_at
)
SELECT 
  title, description, image_url, video_url,
  category, subcategory, location,
  latitude, longitude,
  affiliate_url, affiliate_source,
  is_verified, is_premium,
  rating, likes, views,
  active, created_at, updated_at
FROM json_to_recordset(
  -- Leer FINAL_DATA.json
) AS t(
  title TEXT, description TEXT, image_url TEXT, video_url TEXT,
  category TEXT, subcategory TEXT, location TEXT,
  latitude DECIMAL, longitude DECIMAL,
  affiliate_url TEXT, affiliate_source TEXT,
  is_verified BOOLEAN, is_premium BOOLEAN,
  rating NUMERIC, likes INTEGER, views INTEGER,
  active BOOLEAN, created_at TIMESTAMP, updated_at TIMESTAMP
);

-- Recrear geo_points
UPDATE content
SET geo_point = ST_SetSRID(ST_MakePoint(longitude, latitude), 4326)::geography
WHERE geo_point IS NULL AND latitude IS NOT NULL;

-- Reindex
ANALYZE content;

COMMIT;
```

---

## ‚úÖ CHECKLIST

- [ ] Crear carpeta `/scrape-data/`
- [ ] Instalar dependencias Python
- [ ] Configurar `.env` con SUPABASE_KEY
- [ ] Ejecutar `python scraper.py`
- [ ] Monitorear `/scrape-data/SCRAPE_LOG.txt`
- [ ] Esperar reporte cada 100 registros
- [ ] Validar datos en `/scrape-data/FINAL_DATA.json`
- [ ] Insertar en Supabase
- [ ] Verificar en BD

---

## üÜò SI SE CORTA

**Proceso de reanudaci√≥n:**

1. Lee `checkpoint.json` (dice d√≥nde te quedaste)
2. Ejecuta `python scraper.py` de nuevo
3. Script detecta checkpoint y contin√∫a desde `proxima_categoria`
4. CERO p√©rdida de datos

---

## üìû SOPORTE

Si hay errores:
1. Check `/scrape-data/SCRAPE_LOG.txt`
2. Ajusta selectores HTML en scraper (estructura de sitios puede cambiar)
3. Contacta a Claude si necesitas ayuda con l√≥gica

---

**¬°A TRABAJAR, ANTIGRAVITY!** üöÄ

